\section{Results \label{sec:results}}

Below are the results we got when trying different features

\begin{itemize}
	\item[] \textbf{Benchmark only}: 0.86
	\item[] This is the benchmark from Kaggle. We wrote the same code only in C\# instead of python. This was our final score.
	\item[] \textbf{Topics only}: 0.60
	\item[] In this submission we only used our paper-topic model with no classifier. This approach did not score very well. Using IDF to normalize the the vector improved our score a bit (0,02). Comparing the means of deleted papers and comfirmed papers showed a difference of a little over one standar deviation  
	\item[] \textbf{Graph, year features and benchmark}: 0.78
	\item[] The graph features found using our graph search algorithms.
	\item[] \textbf{Year features and  benchmark}: 0.86
	\item[] Year features found by the original script from kaggle worked just as well as our new year features
\end{itemize}



